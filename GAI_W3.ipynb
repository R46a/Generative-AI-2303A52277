{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz7tx7EOpCaRu5SxE1hRRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R46a/Generative-AI-2303A52277/blob/main/GAI_W3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "01.Write Python code without using any libraries to find the value of x at which the\n",
        "function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f (x) = 5x4 + 3x2 + 10"
      ],
      "metadata": {
        "id": "bKpO29H4DgHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_prime(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    for _ in range(num_iterations):\n",
        "        grad = f_prime(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "initial_x = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
        "print(\"The value of x at which f(x) has a minimum is:\", min_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUIezSNNA7R3",
        "outputId": "6dc3a7ef-781a-4859-a65c-a02f94e98ce0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which f(x) has a minimum is: 4.810419162406747e-28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02.Write Python code without using any libraries to find the value of x and y at which the\n",
        "function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f (x) = 3x2 + 5e−y + 10"
      ],
      "metadata": {
        "id": "-puP1bd4DTDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_derivative_x(x, y):\n",
        "    return 6 * x\n",
        "\n",
        "def partial_derivative_y(x, y):\n",
        "    return -5 * (2.71828 ** -y)\n",
        "\n",
        "def gradient_descent(initial_x, initial_y, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    y = initial_y\n",
        "    for _ in range(num_iterations):\n",
        "        grad_x = partial_derivative_x(x, y)\n",
        "        grad_y = partial_derivative_y(x, y)\n",
        "        x = x - learning_rate * grad_x\n",
        "        y = y - learning_rate * grad_y\n",
        "    return x, y\n",
        "initial_x = 0.5\n",
        "initial_y = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x, min_y = gradient_descent(initial_x, initial_y, learning_rate, num_iterations)\n",
        "print(\"The values of x and y at which g(x, y) has a minimum are:\", min_x, min_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVEWKvAzBl1o",
        "outputId": "8848646a-672c-4df1-a55d-926236d6fb34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values of x and y at which g(x, y) has a minimum are: 6.711561962466847e-28 3.946138890954553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03.Write Python code without using any libraries to find the value of x at which the\n",
        "sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent\n",
        "Algorithm.\n",
        "z(x) = 1/\n",
        "1 + e−x"
      ],
      "metadata": {
        "id": "GzZozYYdDxZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + 2.71828 ** -x)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "\n",
        "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    for _ in range(num_iterations):\n",
        "        grad = sigmoid_derivative(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "\n",
        "initial_x = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
        "print(\"The value of x at which z(x) has a minimum is:\", min_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsQr2-wLBxgl",
        "outputId": "cfd8c269-e36a-40c6-942f-8c957380acbf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which z(x) has a minimum is: -1.6012961162961212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "04.Write Python code without using any libraries to find the value of optimal values of\n",
        "model parameters M and C such that the model’s Square Error Value shown in equation 4 will\n",
        "be minimum. It means model gives output close to expected output\n",
        "SE = (ExpectedOutput − P redictedOutput)2"
      ],
      "metadata": {
        "id": "-5JAVvnjD7xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y_true, M, C):\n",
        "    y_pred = M * x + C\n",
        "    error = y_true - y_pred\n",
        "    gradient_M = -2 * x * error\n",
        "    gradient_C = -2 * error\n",
        "    return gradient_M, gradient_C\n",
        "\n",
        "\n",
        "def gradient_descent(x, y_true, initial_M, initial_C, learning_rate, num_iterations):\n",
        "    M = initial_M\n",
        "    C = initial_C\n",
        "    for _ in range(num_iterations):\n",
        "        total_gradient_M = 0\n",
        "        total_gradient_C = 0\n",
        "        for i in range(len(x)):\n",
        "            grad_M, grad_C = compute_gradients(x[i], y_true[i], M, C)\n",
        "            total_gradient_M += grad_M\n",
        "            total_gradient_C += grad_C\n",
        "        M = M - learning_rate * (total_gradient_M / len(x))\n",
        "        C = C - learning_rate * (total_gradient_C / len(x))\n",
        "    return M, C\n",
        "\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y_true = [3, 7, 11, 15, 19]\n",
        "initial_M = 0.0\n",
        "initial_C = 0.0\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "optimal_M, optimal_C = gradient_descent(x, y_true, initial_M, initial_C, learning_rate, num_iterations)\n",
        "print(\"The optimal values of M and C are:\", optimal_M, optimal_C)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgal9FMTB0Yp",
        "outputId": "924e14fe-0301-43fe-c037-fae31b7929d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal values of M and C are: 3.981660469673651 -0.9337884764204182\n"
          ]
        }
      ]
    }
  ]
}